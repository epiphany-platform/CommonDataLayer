<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Common Data Layer</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Common Data Layer</a></li><li class="chapter-item expanded "><a href="how_it_works.html"><strong aria-hidden="true">2.</strong> How does it work</a></li><li class="chapter-item expanded "><a href="getting_started.html"><strong aria-hidden="true">3.</strong> Getting Started</a></li><li class="chapter-item expanded "><a href="deployment/index.html"><strong aria-hidden="true">4.</strong> Deployment</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deployment/local/index.html"><strong aria-hidden="true">4.1.</strong> Local deployment</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deployment/local/docker-compose.html"><strong aria-hidden="true">4.1.1.</strong> docker-compose</a></li><li class="chapter-item expanded "><a href="deployment/local/helm.html"><strong aria-hidden="true">4.1.2.</strong> Helm</a></li></ol></li><li class="chapter-item expanded "><a href="deployment/production/index.html"><strong aria-hidden="true">4.2.</strong> Production-grade deployment</a></li></ol></li><li class="chapter-item expanded "><a href="schemas_and_views.html"><strong aria-hidden="true">5.</strong> Schemas and Views</a></li><li class="chapter-item expanded "><a href="examples/index.html"><strong aria-hidden="true">6.</strong> Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">6.1.</strong> TODO</div></li></ol></li><li class="chapter-item expanded "><a href="features/index.html"><strong aria-hidden="true">7.</strong> Features</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="features/ordering.html"><strong aria-hidden="true">7.1.</strong> Message ordering</a></li></ol></li><li class="chapter-item expanded "><a href="protocol.html"><strong aria-hidden="true">8.</strong> Protocol schema</a></li><li class="chapter-item expanded "><a href="benchmarks.html"><strong aria-hidden="true">9.</strong> Benchmarks</a></li><li class="chapter-item expanded "><a href="architecture/index.html"><strong aria-hidden="true">10.</strong> Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/management.html"><strong aria-hidden="true">10.1.</strong> Management Layer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/cli.html"><strong aria-hidden="true">10.1.1.</strong> CLI</a></li><li class="chapter-item expanded "><a href="architecture/web_admin.html"><strong aria-hidden="true">10.1.2.</strong> Admin Web Panel</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/api.html"><strong aria-hidden="true">10.2.</strong> GraphQL API</a></li><li class="chapter-item expanded "><a href="architecture/configuration.html"><strong aria-hidden="true">10.3.</strong> Configuration Layer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/schema_registry.html"><strong aria-hidden="true">10.3.1.</strong> Schema Registry</a></li><li class="chapter-item expanded "><a href="architecture/leader_elector.html"><strong aria-hidden="true">10.3.2.</strong> Leader Elector</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/ingestion.html"><strong aria-hidden="true">10.4.</strong> Ingestion Layer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/data_router.html"><strong aria-hidden="true">10.4.1.</strong> Data Router</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/storage.html"><strong aria-hidden="true">10.5.</strong> Storage Layer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/command_service.html"><strong aria-hidden="true">10.5.1.</strong> Command Service</a></li><li class="chapter-item expanded "><a href="architecture/db_shrinker_storage.html"><strong aria-hidden="true">10.5.2.</strong> Db Shrinker Storage</a></li><li class="chapter-item expanded "><a href="architecture/query_service.html"><strong aria-hidden="true">10.5.3.</strong> Query Service</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/retrieval.html"><strong aria-hidden="true">10.6.</strong> Retrieval Layer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/query_router.html"><strong aria-hidden="true">10.6.1.</strong> Query Router</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/utils.html"><strong aria-hidden="true">10.7.</strong> Utils</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">The Common Data Layer</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        <a href="https://github.com/epiphany-platform/CommonDataLayer" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#common-data-layer" id="common-data-layer">Common Data Layer</a></h1>
<p>The Common Data Layer (CDL) is a data storage service. Its primary goals are performance, versatility, scalability, and ease-of-modification.</p>
<h1><a class="header" href="#how-does-it-work" id="how-does-it-work">How does it work</a></h1>
<p>Data intake is all performed over Message Queue and via the Data Router. Message Queue (MQ) is an abstract entity and the CDL currently supports <a href="https://kafka.apache.org/">kafka</a> and <a href="https://www.rabbitmq.com/">RabbitMQ</a>. CDL listens over a single topic queue for messages keyed on strings, each providing a schema ID. The schema ID is used to load the appropriate topic (stored per-schema in the schema registry), which is used to route the message along to the correct repository.</p>
<p>For each repository, a command service is listening to its specific MQ topic for incoming messages. Each message is stored according to the repository's format. Though most of our command service implementations use append-only storage with each value under a key being assigned a version, it is not required by user-implemented command services.</p>
<p>The query router is used to direct requests for data to the appropriate repository. Each repository also has a query service listening for gRPC requests for data. These query services are used for direct queries of data from the repositories. As repositories are meant to be easily introduced to an already running CDL, but the topic per repository can't be used to make a gRPC request, each schema also stores the dynamic address of the query service it belongs to.</p>
<h1><a class="header" href="#getting-started" id="getting-started">Getting Started</a></h1>
<p>For infomration on specific services and their responsibilities:</p>
<ul>
<li><a href="./architecture/command_service.html">Command Service</a></li>
<li><a href="./architecture/data_router.html">Data Router</a></li>
<li><a href="./architecture/schema_registry.html">Schema Registry</a></li>
<li><a href="./architecture/query_service.html">Query Service</a></li>
</ul>
<h1><a class="header" href="#installation" id="installation">Installation</a></h1>
<p>CDL is a written in Rust. See Rust's <a href="https://www.rust-lang.org/tools/install">installation</a> guide to install. Below are the pre-requesites needed to get started: </p>
<ul>
<li>Rust</li>
<li>Docker</li>
<li>Docker Compose</li>
</ul>
<p>You can download <a href="https://docs.docker.com/desktop/">docker desktop</a> for both Windows and MacOS to intall docker and docker compose on your local machine.</p>
<h2><a class="header" href="#working-with-cdl-locally" id="working-with-cdl-locally">Working with CDL Locally</a></h2>
<p>Below is a following simple amount of steps to getting started working with the services in the CDL locally on your machine. To build and install container images of services within the CDL, run <code>build.sh</code> in root directory of this project.</p>
<p>Please review how to set up CDL locally on your machine but viewing <a href="deployment/local/index.html">local setup</a> documentations for a sample deployment. </p>
<p>Below we will walk through a simple use case of the CDL:</p>
<p><strong>Use Case</strong></p>
<ul>
<li>Create Schema</li>
<li>Insert Data</li>
<li>Query Data</li>
</ul>
<h2><a class="header" href="#add-schema-via-cli" id="add-schema-via-cli">Add Schema via CLI</a></h2>
<p>A schema can be added through the CLI tool localed in the <code>cdl-cli</code> directory. To be able to run the cli you must have a rust compiler. The following command below creates the schema with a name according a json schema in a file as well as sets the topic for routing data through kafka. </p>
<pre><code>cargo run --bin cdl -- --registry-addr &lt;registry_address&gt; schema add --name &lt;schema_name&gt; --topic &quot;cdl.document.input&quot; --file &lt;file_path_to_json&gt;
</code></pre>
<p>Here is the sample JSON schema format that the CDL anticipates and ultimatley will validate data by. Please review <a href="./architecture/schema_registry.html">README</a> in <code>schema-registry</code> directory for more information.</p>
<pre><code>{
	&quot;$schema&quot;: &quot;http://json-schema.org/draft-07/schema#&quot;,
    &quot;$id&quot;: &quot;http://example.com/product.schema.json&quot;,
	&quot;definitions&quot;: {
		&quot;1.0.0&quot;: {
            &quot;description&quot;: &quot;A work order&quot;,
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;property1&quot;: {
                    &quot;description&quot;: &quot;&quot;,
                    &quot;type&quot;: &quot;integer&quot;
                },
                &quot;property2&quot;: {
                    &quot;description&quot;:&quot;&quot;,
                    &quot;type&quot;: &quot;string&quot; 
                },
            },
            &quot;required&quot;: [&quot;property1&quot;]
        }
    }
}
</code></pre>
<p><strong>NOTE</strong>: Schema's can be added via <a href="https://grpc.io/docs/what-is-grpc/introduction/">gRPC</a> to the schema registry. Ensure that you have <code>protoc</code> installed on your machine you machine generate <a href="https://github.com/epiphany-platform/CommonDataLayer/tree/develop/crates/rpc/proto">proto</a> files in a supported language and make requests via a client.</p>
<h2><a class="header" href="#insert-data" id="insert-data">Insert Data</a></h2>
<p>Data can be inserted into the system by data being written to Kafka or ingested through RabbitMQ. Data must be in JSON format with the following fields: <code>schemaId</code>, <code>objectId</code> and <code>data</code> to be routed through the CDL. 
It's worth noting that CDL doesn't rely on message key unless <a href="./features/ordering.html">message ordering</a> feature is enabled. However in order to keep system more performant it's advised to pass NULL as message key or evenly distributed strings.</p>
<p>Below is an example of the what input data would look like. Both ID fields are UUIDs.</p>
<pre><code>{
    &quot;schemaId&quot;: &lt;UUID&gt;,
    &quot;objectId&quot;: &lt;UUID&gt;,
    &quot;data&quot;: &quot;{ \&quot;some_propery&quot;: \&quot;object\&quot;}&quot;

}
</code></pre>
<h3><a class="header" href="#publish-messae-via-rabbitmq" id="publish-messae-via-rabbitmq">Publish messae via RabbitMQ</a></h3>
<p>Below is a sample <code>curl</code> command you can also publish a message through RabbitMQ web admin tool through a exchange or directly to a queue. Review <a href="deployment/local/index.html">local setup</a> for configuration details on Kafka, RabbitMQ.</p>
<p>The command below example takes input data and publishes to the default exchange in RabbitMQ. The message gets consumed and is sent to kafka and published to topic which is determined by <code>schemaId</code>.  The message is then routed to command service which handles routing and storage of data by type. </p>
<pre><code>curl -i -u ${user}:${pass} -H &quot;Accept: application/json&quot; \
-H &quot;Content-Type:application/json&quot; \
-XPOST -d'{&quot;properties&quot;:{},&quot;routing_key&quot;:&quot;my_key&quot;,&quot;payload&quot;:&quot;my body&quot;,&quot;payload_encoding&quot;:&quot;string&quot;}'\
http://${ampq_url}/api/exchanges/%2F/${exchange}/publish
</code></pre>
<h2><a class="header" href="#query-data" id="query-data">Query Data</a></h2>
<h3><a class="header" href="#query-via-query-service" id="query-via-query-service">Query via Query Service</a></h3>
<p>Following this example local deployment, you can query for data saved. Here data is saved within the PR.
Ensure that environment variables are set for <code>POSTGRES_USERNAME</code>, <code>POSTGRES_PASSWORD</code>, <code>POSTGRES_HOST</code>, <code>POSTGRES_PORT</code>, <code>POSTGRES_DBNAME</code>, <code>POSTGRES_SCHEMA</code>, <code>INPUT_PORT</code> and <code>DS_QUERY_URL</code> or run query service directly on machine.</p>
<pre><code>cargo run --bin query_service -- \
--schema-registry-addr &lt;registry_addr&gt; \
--ds-query-url &lt;ds-url&gt;  \
--input-port &lt;input_port&gt;
</code></pre>
<h3><a class="header" href="#query-data-via-query-router" id="query-data-via-query-router">Query Data via Query Router</a></h3>
<p>The Query Router works with the query services to route requests to the correct repository, determined per schema (based on its query address).</p>
<pre><code>cargo run --bin query_router -- \
--schema-registry-addr &lt;schema_registry_addr&gt; \
--cache-capacity &lt;cache_capacity&gt; \
--input-port &lt;input_port&gt;
</code></pre>
<h2><a class="header" href="#deployment" id="deployment">Deployment</a></h2>
<p>See <a href="./deployment/index.html">chapter in the book</a></p>
<h1><a class="header" href="#deployment-1" id="deployment-1">Deployment</a></h1>
<ul>
<li><a href="deployment/local/index.html">Local deployment</a></li>
<li><a href="deployment/production/index.html">Production-grade deployment</a></li>
</ul>
<h1><a class="header" href="#local-deployment" id="local-deployment">Local deployment</a></h1>
<p>Currently CDL supports two ways of local deployment:
via <a href="deployment/local/helm.html">HELM chart</a> and via <a href="deployment/local/docker-compose.html">docker-compose</a></p>
<h1><a class="header" href="#docker-compose" id="docker-compose">docker-compose</a></h1>
<h2><a class="header" href="#preamble" id="preamble">Preamble</a></h2>
<blockquote>
<p>Intended way of deploying CDL is through helm files.</p>
</blockquote>
<p>Contents of this folder aren't meant for use on production and they may be lagging behind our k8s deployment. 
Sole purpose of this directory is to prepare exemplary development environment, from which anyone can startup their development on 
<code>common data layer</code> without Kubernetes knowledge. Contents of docker-compose may not contain all applications, so be aware of that. You may alter it
on your local machine to your needs.</p>
<p>For k8s deployment, please refer to our <a href="deployment/local/helm.html">documentation</a>. </p>
<h2><a class="header" href="#requirements" id="requirements">Requirements</a></h2>
<ul>
<li>docker</li>
<li>docker-compose</li>
<li>rust (optionally)</li>
</ul>
<h2><a class="header" href="#volume" id="volume">Volume</a></h2>
<p>The directory <code>./docker-volume</code> is used as a volume. Please note it is not fully <code>.gitignore</code>d because we rely on some setup scripts attached via volumes.</p>
<h2><a class="header" href="#deployment-2" id="deployment-2">Deployment</a></h2>
<p>You must first add environment variables:</p>
<p><code>DOCKER_BUILDKIT=1</code><br />
<code>COMPOSE_DOCKER_CLI_BUILD=1</code></p>
<p>Environment with infrastructure alone is started via:</p>
<p><code>docker-compose up -d</code></p>
<p>If you want to add cdl components to it, you must specify <code>-f</code> options:</p>
<p><code>docker-compose -f docker-compose.cdl-kafka.yml -f docker-compose.yml up -d</code>
or
<code>docker-compose -f docker-compose.cdl-rabbit.yml -f docker-compose.yml up -d</code></p>
<p>Sometimes it's useful to store data on disk (eg. for debugging), we can achieve this by adding <code>-f docker-compose.host-storage.yml</code> to combination:</p>
<p><code>docker-compose -f docker-compose.host-storage.yml -f docker-compose.yml up -d</code></p>
<h2><a class="header" href="#entry-points-in-system" id="entry-points-in-system">Entry points in system</a></h2>
<h3><a class="header" href="#kafka" id="kafka">Kafka</a></h3>
<p>You can write to kafka on <code>localhost:9092</code>.
Default <em>data-router</em> topic is <code>cdl.data.input</code>.
By default there is no replication on <em>schema_registry</em>. Postgres <em>command_service</em> input channel is <code>cdl.document.data</code>.</p>
<p>Errors are written to <code>cdl.reports</code>.</p>
<h3><a class="header" href="#rabbitmq" id="rabbitmq">Rabbitmq</a></h3>
<p>You can write to rabbit on <code>localhost:5672</code>.
Default <em>data-router</em> fanout exchange is <code>cdl.data.input</code>.
There is also managament panel available at <code>localhost:15672</code>. The credentials are <code>user</code>/<code>CHANGEME</code>.
By default there is no replication on <em>schema_registry</em>. Postgres <em>command_service</em> input channel is <code>cdl.document.data</code>.</p>
<p>Errors are written to <code>cdl.reports</code> fanout exchange and can be read via <code>cdl.reports</code> queue.</p>
<h3><a class="header" href="#postgresql" id="postgresql">PostgreSQL</a></h3>
<p>To access postgres you must have some postgresql client installed.</p>
<p>For command line it's best to refer to your OS package manager (<code>homebrew</code> on OSX, <code>apt</code> on Ubuntu, <code>choco</code> on Windows).</p>
<p><code>psql -U postgres --password -h localhost</code>
the password is <code>1234</code></p>
<h3><a class="header" href="#schema-registry" id="schema-registry">Schema registry</a></h3>
<p>Schema registry can be either accessed via <a href="https://github.com/epiphany-platform/CommonDataLayer/blob/develop/crates/rpc/proto/schema_registry.proto">gRPC</a>, or via <code>cdl-cli</code>. Using <code>cdl-cli</code> will require presence of rust compiler on your local machine.
Tips on how to install rust are available on <a href="https://rustup.rs/">rustup website</a>.</p>
<p>From main directory of this project you can run <code>cdl-cli</code> via:</p>
<p><code>cargo run -p cdl-cli -- --help</code></p>
<p>Registry address is <code>http://localhost:50101</code>.</p>
<p>eg.</p>
<ul>
<li>Adding new schema:</li>
</ul>
<blockquote>
<p><code>cargo run -p cdl-cli -- --registry-addr &quot;http://localhost:50101&quot; schema add --name default-document</code></p>
</blockquote>
<ul>
<li>Setting schema topic (in order for this schema to be routed to <code>command-service</code> topic must be <code>cdl.document.input</code>)</li>
</ul>
<blockquote>
<p><code>cargo run -p cdl-cli -- --registry-addr &quot;http://localhost:50101&quot; schema set-topic --id 0a626bba-15ff-11eb-8004-000000000000 --topic &quot;cdl.document.input&quot;</code></p>
</blockquote>
<ul>
<li>Getting all schemas</li>
</ul>
<blockquote>
<p><code>cargo run -p cdl-cli -- --registry-addr &quot;http://localhost:50101&quot; schema names</code></p>
</blockquote>
<h1><a class="header" href="#recipes" id="recipes">Recipes</a></h1>
<h2><a class="header" href="#druid-timeseries-env" id="druid-timeseries-env">Druid timeseries env</a></h2>
<pre><code>docker-compose -f docker-compose.cdl-kafka.yml -f docker-compose.yml -f docker-compose.druid.yml up -d \
    postgres \
    zoo_kafka \
    kafka \
    zoo_druid \
    coordinator \
    broker \
    historical \
    router \
    middlemanager \
    schema_registry \
    data_router \
    druid_command \
    druid_query \
    query_router
</code></pre>
<h1><a class="header" href="#helm" id="helm">Helm</a></h1>
<p>For most use cases using docker-compose is a good way for local development. However some features can be developed/tested only inside Kubernetes cluster environments.</p>
<h2><a class="header" href="#requirements-1" id="requirements-1">Requirements</a></h2>
<ul>
<li><a href="https://docs.docker.com/engine/install/">docker</a>, <a href="https://docs.docker.com/compose/install/">docker-compose</a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">minikube</a> (other type of local Kubernetes cluster may be used, but some commands may differ)</li>
<li><a href="https://helm.sh/docs/intro/install/">helm</a></li>
</ul>
<h2><a class="header" href="#setting-up-local-cluster" id="setting-up-local-cluster">Setting up local cluster</a></h2>
<h3><a class="header" href="#start-the-cluster" id="start-the-cluster">Start the cluster</a></h3>
<p>Decide how much resources can be used by the local k8s cluster and start it. </p>
<p>Note: Resources aren't blocked if k8s cluster is idle, parent system can still use them. It's recommended to use all/almost all of available cpus because rust compilation takes place in this environment(and it can take a while especially on a release build).</p>
<p><code>minikube start --cpus 8 --memory 8192 --driver=docker</code></p>
<h2><a class="header" href="#building-docker-image" id="building-docker-image">Building docker image</a></h2>
<p>Next step is to build docker images which will be used by k8s pods. </p>
<pre><code class="language-shell">eval $(minikube docker-env)
ENV=PROD DOCKER_BUILDKIT=1 ./build.sh
</code></pre>
<p>First command will change docker daemon we're communicating with to docker inside minikube. From now on any docker command run from current shell will be executed inside minikube docker daemon. To connect to your standard docker daemon just start a new shell.
Second command builds docker image, it may take some time first time you build the image. You can change <code>ENV=PROD</code> to <code>ENV=DEV</code> if you want shorter build time(in cost of less performant output).</p>
<h2><a class="header" href="#spin-up-infrastructure-services" id="spin-up-infrastructure-services">Spin up infrastructure services</a></h2>
<p>To start necessary infrastructure(not necessary if you've deployed infrastructure yourself/you want to connect to services on our azure cluster):</p>
<p><code>helm install --values ./deployment/helm/infrastructure/values.yaml infrastructure ./deployment/helm/infrastructure</code></p>
<p>If you want to use druid repository you also need to start druid.
<code>helm install --values ./deployment/helm/infrastructure-druid/values.yaml infrastructure-druid ./deployment/helm/infrastructure-druid</code></p>
<h2><a class="header" href="#installing-cdl" id="installing-cdl">Installing CDL</a></h2>
<p>To install the solution you need to execute:</p>
<p><code>helm install --values ./helm/cdl/values-local.yaml cdl ./deployment/helm/cdl</code></p>
<p>After a moment Kubernetes pods should get started. You can check their status by <code>kubectl get pods</code></p>
<p>Note: Using default <code>values.yaml</code> file(or skipping this parameter) will install configuration for our cloud cluster.</p>
<h2><a class="header" href="#removing-cdl" id="removing-cdl">Removing CDL</a></h2>
<p>To remove current installation of CDL you can use:</p>
<p><code>helm uninstall cdl</code></p>
<p>This operation will take ~30 seconds(default Kubernetes timeout).</p>
<h2><a class="header" href="#upgrading-cdl" id="upgrading-cdl">Upgrading CDL</a></h2>
<p>The easiest way to update whole deployment is to uninstall it, rebuild docker image and reinstall the helm chart.</p>
<h2><a class="header" href="#useful-commands" id="useful-commands">Useful commands</a></h2>
<ul>
<li><code>minikube list services</code> - list services running on minikube(port numbers for input, output etc.) </li>
<li><code>minikube dashboard</code> - runs web dashboard of the Kubernetes cluster</li>
<li><code>kubectl get pods</code> - get list of pods</li>
<li><code>kubectl exec cdl-rust-storage-0 -it -- /bin/ash</code> - run commands directly on single k8s pod</li>
<li><code>kubectl logs cdl-rust-storage-0</code> - get logs generated by pod</li>
<li><code>kubectl describe pod cdl-rust-storage-0</code> - gets more information about a pod, may contain info why pod is not starting etc.</li>
</ul>
<h2><a class="header" href="#troubleshooting" id="troubleshooting">Troubleshooting</a></h2>
<p>Few problems you might encounter during development on local k8s cluster:</p>
<h3><a class="header" href="#tls-errors-while-running-docker-commands" id="tls-errors-while-running-docker-commands">TLS errors while running docker commands</a></h3>
<p>Sometimes(if minikube isn't properly stopped) it changes cluster address. This results with errors while trying to connect to minikube docker daemon. In order to fix it we have to restart the daemon.
<em>This issue should be fixed on current minikube version.</em></p>
<h1><a class="header" href="#production-grade-deployment" id="production-grade-deployment">Production-grade deployment</a></h1>
<p>TODO</p>
<h1><a class="header" href="#schemas-and-views" id="schemas-and-views">Schemas and Views</a></h1>
<h2><a class="header" href="#schemas" id="schemas">Schemas</a></h2>
<p>Schemas are the format in which data is to be sent to the Common Data Layer. Each schema is assigned
a random UUID on creation and initially is created with a name and an initial definition. The name is
not required to be unique among all schemas (as the UUID is the unique identifier of schemas), but is
simply for identifying the schema when searching for schemas. It can be updated at any time.</p>
<p>The definition is a <a href="https://json-schema.org/">JSON Schema</a> document that describes the expected format of data stored
under the given schema. It is assigned the semantic version 1.0.0, and cannot be updated after creation.
Rather, updates can be made to the definition by inserting another definition with a new semantic version
strictly larger than any other existing version assigned to that schema.</p>
<p>When validating data against a schema, either the latest version of the definition is used, or optionally 
a semantic version range can be provided, and the latest version meeting the range is used.</p>
<p>Schemas can also have multiple views, described below.</p>
<p>An example of a CDL schema might be: </p>
<pre><code class="language-json">{
    &quot;id&quot;: &quot;&lt;schema UUID&gt;&quot;,
    &quot;name&quot;: &quot;Vector&quot;,
    &quot;definitions&quot;: {
        &quot;1.0.0&quot;: {
            &quot;x&quot;: &quot;number&quot;,
            &quot;y&quot;: &quot;number&quot;,
            &quot;z&quot;: &quot;number&quot;
        },
        &quot;1.0.0&quot;: {
            &quot;w&quot;: &quot;number&quot;,
            &quot;x&quot;: &quot;number&quot;,
            &quot;y&quot;: &quot;number&quot;,
            &quot;z&quot;: &quot;number&quot;
        }
    },
    &quot;views&quot;: [
        &quot;&lt;view 1 UUID&gt;&quot;,
        &quot;&lt;view 2 UUID&gt;&quot;,
        &quot;&lt;view 3 UUID&gt;&quot;
    ]
}
</code></pre>
<h2><a class="header" href="#views" id="views">Views</a></h2>
<p>Views describe projections of data defined by a specific schema. As with schemas, each view is assigned
a UUID on creation and initially is created with a name and an initial definition. Like schemas, the
name is a vanity name for searching purposes only and can be updated at any time, as the UUID is the unique
identifier used. On creation, a view is assigned to a schema and cannot be assigned to a different one.</p>
<p>The definition is a <a href="https://jmespath.org/">JMESPath</a> expression which describes how to project data defined under the
parent schema into the desired output format. Unlike with schemas, view definitions are editable at any time
and are not versioned, though this feature may be added in the future.</p>
<p>An example of a CDL view might be:</p>
<pre><code class="language-json">{
    &quot;id&quot;: &quot;&lt;view UUID&gt;&quot;,
    &quot;name&quot;: &quot;two dimensions&quot;,
    &quot;definition&quot;: &quot;{ x: x, y: y }&quot;
}
</code></pre>
<h1><a class="header" href="#examples" id="examples">Examples</a></h1>
<p>TODO</p>
<h1><a class="header" href="#features" id="features">Features</a></h1>
<ul>
<li><a href="features/ordering.html">Message ordering</a></li>
</ul>
<h1><a class="header" href="#message-ordering" id="message-ordering">Message ordering</a></h1>
<h2><a class="header" href="#index" id="index">Index</a></h2>
<ul>
<li><a href="features/ordering.html#what_is">What is message ordering</a></li>
<li><a href="features/ordering.html#why">Why does it matter</a>
<ul>
<li><a href="features/ordering.html#soccer">Football/soccer game</a></li>
<li><a href="features/ordering.html#trafic_monitor">Network traffic monitor</a></li>
<li><a href="features/ordering.html#pros">Pros and cons</a></li>
</ul>
</li>
<li><a href="features/ordering.html#message_ordering_in_cdl">Message ordering in CDL</a></li>
<li><a href="features/ordering.html#how_to_use">How to use it</a>
<ul>
<li><a href="features/ordering.html#overall">Overall</a></li>
<li><a href="features/ordering.html#kafka">Communication through Apache Kafka</a></li>
<li><a href="features/ordering.html#rabbit">Communication through RabbitMQ</a></li>
<li><a href="features/ordering.html#rpc">Communication through RPC</a></li>
</ul>
</li>
</ul>
<h2><a class="header" href="#a-namewhat_isawhat-is-message-ordering" id="a-namewhat_isawhat-is-message-ordering"><a name="what_is"></a>What is message ordering</a></h2>
<p>Message ordering is a guarantee that certain messages will be processed by CDL in the same order there were sent to the system. Proper ordering matter mostly to user applications which base their business logic on real-time data where no approximation is allowed. </p>
<h2><a class="header" href="#a-namewhyawhy-does-it-matter" id="a-namewhyawhy-does-it-matter"><a name="why"></a>Why does it matter</a></h2>
<p>Let's consider two use cases which will show us why order of received messages might/might not matter: </p>
<h3><a class="header" href="#a-namesoccerafootballsoccer-game" id="a-namesoccerafootballsoccer-game"><a name="soccer"></a>Football/soccer game</a></h3>
<p>We develop an application which decides strategy for playing football matches. Based on actual score we decide if our team should play more offensively, defensively or utilize balanced play style. </p>
<p>Incoming events: </p>
<ul>
<li>(1) Match start </li>
<li>(2) We scored a goal </li>
<li>(3) Enemy scored a goal </li>
<li>(4) Match end </li>
</ul>
<p>Linearizable system (system which can establish exact order for each message), will see events in the order they were produced. Our application will start game with balanced play style, go defense after 2nd event is received (we scored a goal) and change it back to balanced once enemy hits us back. </p>
<p>Without proper message ordering same situation can be processed (seen by clients) differently. If second message got delayed for some reason and came out of order, we could play with completely different strategy. We would start game in balanced formation, then the 3rd message will show up (enemy scored a goal), so we'll think we're losing the game and start playing offensively. After some time, 2nd message will finally show up and we'll end game in balanced formation. </p>
<p>From user perspective it would seem that our program is broken because we played more offensively once we were one point ahead of the enemy, which could cause us to lose our advantage. </p>
<p>It's worth noting that if we play other sport discipline e.g., a basketball or volleyball making decisions with few delayed messages can be considered valid behavior - there are more data points, so it would be fine to use approximated score (difference of one or two points doesn't change the general strategy because there are much more points in general). In such case we don't make our decision based on loosing single point but make it if we're losing by a few points. Loosing single point (single message) has a small impact on our decision-making process. </p>
<h3><a class="header" href="#a-nametrafic_monitoranetwork-traffic-monitor" id="a-nametrafic_monitoranetwork-traffic-monitor"><a name="trafic_monitor"></a>Network traffic monitor</a></h3>
<p>We develop an application which measure network traffic. It can show different statistics per chosen period. </p>
<p>We're looking for statistics, so it's often fine to approximate the data. We mostly do our job on many data points at once (time period) so even skipping some of them would mostly be fine. What is also important is the fact that we're not processing a real-time data - we're showing data from some time ago (a month, an hour etc.), so even if messages come in the wrong order, they will be available in the system once we query them. </p>
<p>In this case message ordering is not that important. We're using historical data which will be correct regardless of message ordering.</p>
<h3><a class="header" href="#a-nameprosapros" id="a-nameprosapros"><a name="pros"></a>Pros</a></h3>
<ul>
<li>without message ordering system might see a state of things that have never happened (not just delayed state) </li>
</ul>
<h3><a class="header" href="#a-nameconsacons" id="a-nameconsacons"><a name="cons"></a>Cons</a></h3>
<ul>
<li>fully linearizable systems can be really slow - inerrability drastically limits system ability to process data in parallel (and scale horizontally) </li>
</ul>
<h2><a class="header" href="#a-namemessage_ordering_in_cdlamessage-ordering-in-cdl" id="a-namemessage_ordering_in_cdlamessage-ordering-in-cdl"><a name="message_ordering_in_cdl"></a>Message ordering in CDL</a></h2>
<p>CDL supports three message ordering strategies: </p>
<ul>
<li>Fully ordered messages(linearizable) </li>
<li>Message ordering defined by causality </li>
<li>Unordered messages (no message ordering guarantees) </li>
</ul>
<p>Message ordering defined by causality is a middle ground between two opposite strategies. It allows you to keep message ordering for some of the messages without performance costs of full linearization. E.g., in our first example we could say that order of messages regarding same game is important, but we don't care about order of two messages related to different sport events. </p>
<h2><a class="header" href="#a-namehow_to_useahow-to-use-it" id="a-namehow_to_useahow-to-use-it"><a name="how_to_use"></a>How to use it</a></h2>
<h3><a class="header" href="#a-nameoverallaoverall" id="a-nameoverallaoverall"><a name="overall"></a>Overall</a></h3>
<p>In CDL message ordering guarantees are defined on per message level. In CDL data ingestion message format there is an optional field called <code>order_group_id</code>. This field should contain user generated UUID with ordering info. If you set this value CDL guarantees that messages with the same <code>order_group_id</code> will be processed in the order they were send to CDL. Otherwise, if field is left empty, no ordering guarantees are met for this message. This behavior allows us to support causality ordering (multiple <code>order_group_id</code> values), linearizability (same <code>order_group_id</code> for each message) or to skip ordering guarantees at all(<code>ordering_group_id</code> not provided). </p>
<h3><a class="header" href="#a-namekafkaacommunication-through-apache-kafka" id="a-namekafkaacommunication-through-apache-kafka"><a name="kafka"></a>Communication through Apache Kafka</a></h3>
<p>If you’re using Kafka as a message bus following requirements needs to be met for message ordering to work correctly: </p>
<ul>
<li>Kafka partitioning should be based on message key </li>
<li>Message keys of data coming to CDL should be set to <code>order_group_id</code> or left empty if message order is not important</li>
<li>Scaling data router and command service is possible up to number of Kafka partitions. If you need more service instances you must have enough Kafka partitions to feed them messages. </li>
</ul>
<h3><a class="header" href="#a-namerabbitacommunication-through-rabbitmq" id="a-namerabbitacommunication-through-rabbitmq"><a name="rabbit"></a>Communication through RabbitMQ</a></h3>
<p>If you’re using RabbitMQ as a message bus following requirements needs to be met for message ordering to work correctly: </p>
<ul>
<li>You must create proper exchange-queue bindings in RabbitMQ 
<ul>
<li>create 2+ queues - one for unordered messages, one or more for ordered ones; number of queues is the limit of horizontal instance scaling (exception - unordered messages) </li>
<li>create one exchange which name will be saved in schema registry</li>
<li>create binding between exchange and queues in a way that messages with <code>unordered</code> message key will go to queues with unordered data, other keys will be split between other queues </li>
</ul>
</li>
<li>Configure command service instances: 
<ul>
<li>Unordered message queue can be passed to each command service instance </li>
<li>Ordered message queue can be passed to single command service (exclusive consumer) </li>
</ul>
</li>
<li>Message keys of data coming to CDL should be set to <code>order_group_id</code> or left empty if message order is not important</li>
</ul>
<p>Unfortunately, that means that scaling command services can be done only manually (automatic scaling may be implemented by <a href="https://github.com/epiphany-platform/CommonDataLayer/issues/185">#185</a>), instances which process only unordered messages can be scaled automatically. </p>
<h3><a class="header" href="#a-namerpcacommunication-through-rpcwip" id="a-namerpcacommunication-through-rpcwip"><a name="rpc"></a>Communication through RPC(WIP)</a></h3>
<p>In case of communication through RPC message ordering is guaranteed by request/response pattern and its client responsibility to decide if messages can be sent (and processed) in parallel. <code>order_group_id</code> field is ignored. </p>
<h1><a class="header" href="#protocol-schema" id="protocol-schema">Protocol schema</a></h1>
<p>TODO</p>
<h1><a class="header" href="#benchmarks" id="benchmarks">Benchmarks</a></h1>
<p>TODO</p>
<h1><a class="header" href="#architecture" id="architecture">Architecture</a></h1>
<p>The CDL consists of five layers, each horizontally scalable and replaceable.</p>
<p><img src="architecture/../mdbook-plantuml-img/c771d5433e2b68ed4931140862f2496ef27227be.svg" alt="" /></p>
<h2><a class="header" href="#management-layer" id="management-layer">Management Layer</a></h2>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td><a href="architecture/cli.html">cdl-cli</a></td><td>Provides a command-line interface for managing schemas in the schema registry and storing and retrieving data</td></tr>
<tr><td><a href="architecture/web_admin.html">web-admin</a></td><td>Admin Web Panel - provides GUI interface for managing schemas and storing and retrieving data</td></tr>
</tbody></table>
<h2><a class="header" href="#graphql-api" id="graphql-api">GraphQL API</a></h2>
<p><a href="architecture/api.html">API</a> - used as a backend service for web-admin, provides unified interface to manage CDL.</p>
<h2><a class="header" href="#configuration-layer" id="configuration-layer">Configuration Layer</a></h2>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td><a href="architecture/schema_registry.html">schema-registry</a></td><td>Manage user-defined schemas that define the format of incoming values and their respective topics</td></tr>
<tr><td><a href="architecture/leader_elector.html">leader-elector</a></td><td>Elect master nodes in replicated services (<em>only for the Schema Repository, currently</em>)</td></tr>
</tbody></table>
<h2><a class="header" href="#ingestion-layer" id="ingestion-layer">Ingestion Layer</a></h2>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td><a href="architecture/data_router.html">data-router</a></td><td>Route incoming data from and through MQ for consumption by the specific Command Service</td></tr>
</tbody></table>
<h2><a class="header" href="#storage-layer" id="storage-layer">Storage Layer</a></h2>
<p>Storage layer, which is sometimes called &quot;repository&quot;.</p>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td><a href="architecture/query_service.html">query-service</a></td><td>Wrap each individual database for retrieval of data</td></tr>
<tr><td><a href="architecture/command_service.html">command-service</a></td><td>Intake data from a MQ and storage, in specific database</td></tr>
<tr><td><a href="architecture/db_shrinker_storage.html">db-shrinker-storage</a></td><td>A service to remove older data from storage</td></tr>
</tbody></table>
<h2><a class="header" href="#retrieval-layer" id="retrieval-layer">Retrieval Layer</a></h2>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td><a href="architecture/query_router.html">query-router</a></td><td>Route incoming requests to query service based on schema id</td></tr>
</tbody></table>
<h2><a class="header" href="#additional-crates" id="additional-crates">Additional crates</a></h2>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td>rpc</td><td>A collection of GRPC proto files and automatically generated client/server code.</td></tr>
<tr><td>utils</td><td>A collection of utilities used throughout the Common Data Layer</td></tr>
</tbody></table>
<h2><a class="header" href="#useful-directories" id="useful-directories">Useful directories</a></h2>
<table><thead><tr><th>Directory</th><th>Purpose</th></tr></thead><tbody>
<tr><td>deploy/helm</td><td>helm charts for kubernetes deployment</td></tr>
<tr><td>deploy/compose</td><td>sample deployment guide for docker (development-only)</td></tr>
<tr><td>benchmarking</td><td>scripts and scaffolding data for benchmarking</td></tr>
<tr><td>tests</td><td>component tests</td></tr>
<tr><td>examples</td><td>examplary client of cdl</td></tr>
<tr><td>docs</td><td>cdl documentation</td></tr>
</tbody></table>
<h1><a class="header" href="#management-layer-1" id="management-layer-1">Management Layer</a></h1>
<p>Consists of services and tools responsible for manipulating and managing CDL.</p>
<p>CDL provides two different User Interfaces:</p>
<ul>
<li><a href="architecture/./cli.html">CLI</a></li>
<li><a href="architecture/./web_admin.html">Admin Web Panel</a></li>
</ul>
<h1><a class="header" href="#cli" id="cli">CLI</a></h1>
<h2><a class="header" href="#technical-description" id="technical-description">Technical Description</a></h2>
<p>The CDL-CLI is the official tool for interacting with the CDL's Schema Registry, used both for viewing and manipulating schemas and their respective data.</p>
<p>For this tool to work, please make sure that the Schema Registry's gRPC server is listening on a public port. Currently, the Schema Registry only exposes a gRPC API, which is faster than a JSON API but less convenient to use. There is some progress with a JSON API for convenience, as well as a TUI (terminal user interface) and a website.</p>
<p>Communication Methods:</p>
<ul>
<li>GRPC</li>
</ul>
<h2><a class="header" href="#how-to-guide" id="how-to-guide">How to guide</a></h2>
<p>For the sake of concision, though you will probably be running <code>cargo run --bin cdl -- &lt;options&gt;</code>,
this README will simply describe commands with the shorthand <code>cdl &lt;options&gt;</code>.</p>
<p><em>Note: This assumes you are running the common data layer locally for now. The ports for</em>
<em>schema registry and the storage service are copied from the <code>docker-compose.yml</code> file, but</em>
<em>if you are using different ports, you should provide those with the <code>--port</code> option.</em></p>
<h4><a class="header" href="#manipulate-views" id="manipulate-views">Manipulate Views</a></h4>
<p>To add a view under a schema already defined in the registry, run
<code>cdl schema views -s &lt;schema_name&gt; add -n &lt;view_name&gt; -v &lt;JMESPath_view&gt;</code>. Views can only be added
to schemas if they do not already exist on the schema; the <code>update</code> command can be used the same way
as <code>add</code> to update existing schema views.</p>
<p><em>Make sure that views are valid <a href="https://jmespath.org/">JMESPath</a> expressions.</em></p>
<p>To list all views of a schema alphabetically, run <code>cdl schema views -s &lt;schema_name&gt; names</code>.</p>
<p>To get a specific view on a schema, run <code>cdl schema views -s &lt;schema_name&gt; get -n &lt;view_name&gt;</code>.</p>
<h4><a class="header" href="#manipulate-schemas" id="manipulate-schemas">Manipulate Schemas</a></h4>
<h6><a class="header" href="#add-schema" id="add-schema">Add Schema</a></h6>
<p><code>cdl --registry-address &quot;http://localhost:6400 schema &lt;add|get|names|update&gt; --name &lt;schemaname&gt; \ --query-address &lt;query-service-uri&gt;&quot; \ --topic &lt;ingest-topic&gt; \ --file &lt;optional:schema-path&gt; </code></p>
<ul>
<li>If <code>--file</code> is provided, the specified file must have valid JSON inside.</li>
<li>If <code>--file</code> is missing, the CLI will expect JSON to be piped in over <code>stdin</code>.</li>
<li>A schema containing <code>true</code> will accept any valid JSON data.</li>
<li>New schemas are assigned a random UUID on creation, which will be printed after a successful insert.</li>
</ul>
<h6><a class="header" href="#list-schemas" id="list-schemas">List Schemas</a></h6>
<p>To print all existing schema names and their respective ID's:
<code>cdl --registry-address &quot;http://localhost:6400 schema names</code></p>
<h1><a class="header" href="#admin-web-panel" id="admin-web-panel">Admin Web Panel</a></h1>
<p>This is the management portal for the CDL, useful for updating
configuration, manipulating data.</p>
<h2><a class="header" href="#setup" id="setup">Setup</a></h2>
<p>This site is written with <a href="https://svelte.dev/">Svelte.JS</a> and <a href="https://www.typescriptlang.org/">TypeScript</a>.
To run or develop this site, you'll need to install <a href="https://npmjs.com/">NPM</a>:
I recommend using a version manager to install the latest version
like <a href="https://github.com/Schniz/fnm">fnm</a>, a Rust-based version manager for NPM.</p>
<p>Once you have NPM in your path, run <code>npm i</code> in this directory to
install all package dependencies.</p>
<h2><a class="header" href="#running" id="running">Running</a></h2>
<p>For development, the command <code>npm run dev</code> will run a dev server
on localhost:5000 (or a random port if 5000 is taken) which you
can access from your local browser.</p>
<h2><a class="header" href="#deployment-3" id="deployment-3">Deployment</a></h2>
<p>This site is deployed as a plain folder, specifically the <code>public</code>
folder in the root of this repo. Before deploying that folder, make
sure to run <code>npm run build</code> to build an optimized version of this
site and save it to the <code>public</code> directory.</p>
<h1><a class="header" href="#graphql-api-1" id="graphql-api-1">GraphQL API</a></h1>
<p>Server which provides <code>/graphql</code> and <code>/graphiql</code> routes for CDL management.
It is self-describing, interactive and easy to use way to manage your instance.</p>
<h1><a class="header" href="#getting-started-on-local-machine-via-docker-compose" id="getting-started-on-local-machine-via-docker-compose">Getting started on local machine (via docker-compose)</a></h1>
<p>Check our <a href="architecture/../deployment/local/docker-compose.html">guide</a> to see how to deploy API locally.</p>
<p>You can access interactive graphQL editor at http://localhost:50106/graphiql. It supports auto-completion, has built-in documentation explorer and history. </p>
<p>Because our schema-registry in docker-compose is automatically initialized with some schemas, you can start making queries right away, like:</p>
<pre><code class="language-graphql">{
    schemas {
      id,
      definitions {
        version,
        definition
      },
      views {
        expression
      }
    }
}
</code></pre>
<h3><a class="header" href="#configuration-environment-variables" id="configuration-environment-variables">Configuration (Environment Variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INPUT_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
<tr><td>SCHEMA_REGISTRY_ADDR</td><td>Address of schema registry gRPC API</td><td>http://schema_registry:50101</td><td>yes</td><td></td></tr>
<tr><td>QUERY_ROUTER_ADDR</td><td>Address of query router gRPC API</td><td>http://query_router:50101</td><td>yes</td><td></td></tr>
<tr><td>COMMUNICATION_METHOD</td><td>The method of communication with external services</td><td><code>kafka</code> / <code>amqp</code> / <code>grpc</code></td><td>yes</td><td></td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#kafka-configuration" id="kafka-configuration">Kafka Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>kafka</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>KAFKA_BROKERS</td><td>Address to Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td></td></tr>
<tr><td>KAFKA_GROUP_ID</td><td>Group ID of the consumer</td><td><code>postgres_command</code></td><td>yes</td><td></td></tr>
<tr><td>REPORT_SOURCE</td><td>Kafka topic on which API listens for notifications</td><td><code>cdl.notifications</code></td><td>yes</td><td></td></tr>
<tr><td>INSERT_DESTINATION</td><td>Kafka topic to which API inserts new objects</td><td><code>cdl.data.input</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#amqp-configuration" id="amqp-configuration">AMQP Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>amqp</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>AMQP_CONNECTION_STRING</td><td>Connection URL to AMQP Server</td><td><code>amqp://user:CHANGEME@rabbitmq:5672/%2f</code></td><td>yes</td><td></td></tr>
<tr><td>AMQP_CONSUMER_TAG</td><td>Consumer tag</td><td><code>postgres_command</code></td><td>yes</td><td></td></tr>
<tr><td>REPORT_SOURCE</td><td>AMQP queue on which API listens for notifications</td><td><code>cdl.notifications</code></td><td>yes</td><td></td></tr>
<tr><td>INSERT_DESTINATION</td><td>AMQP exchange to which API inserts new objects</td><td><code>cdl.data.input</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#grpc-configuration" id="grpc-configuration">gRPC Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>grpc</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INSERT_DESTINATION</td><td>gRPC service address on which API inserts new objects</td><td><code>http://data_router:50101</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h1><a class="header" href="#configuration-layer-1" id="configuration-layer-1">Configuration Layer</a></h1>
<p>Consists of services responsible for holding state and configuration of CDL.<br />
Currently only the Schema Registry resides here, which keeps information about schemas and views.</p>
<h1><a class="header" href="#schema-registry-1" id="schema-registry-1">Schema Registry</a></h1>
<h3><a class="header" href="#technical-description-1" id="technical-description-1">Technical Description</a></h3>
<p>The Schema Registry (<code>SR</code> for short) is responsible for storing configuration about the data types handled by CDL. It is a persistent graph database, that can be queried via gRPC (other means of interaction are in progress). Currently there is no GUI nor TUI; user interaction is currently performed with the <a href="architecture/cli.html">CDL-CLI</a>. Replication across multiple instances of the Schema Registry is supported.</p>
<p>Interacts with:</p>
<ul>
<li>nothing on its own</li>
</ul>
<p>Is used by:</p>
<ul>
<li>Data Router</li>
<li>Query Router</li>
<li>cdl-cli</li>
</ul>
<p>Query methods:</p>
<ul>
<li>gRPC (clients may use cdl-cli CLI application)</li>
</ul>
<p>Communication methods (supported repositores):</p>
<ul>
<li>Kafka (with other schema-registry instances)</li>
</ul>
<h3><a class="header" href="#configuration-environment-variables-1" id="configuration-environment-variables-1">Configuration (Environment Variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INPUT_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
<tr><td>COMMUNICATION_METHOD</td><td>The method of communication with external services</td><td><code>kafka</code> / <code>amqp</code> / <code>grpc</code></td><td>yes</td><td></td></tr>
<tr><td>REPLICATION_ROLE</td><td>(deprecated)</td><td><code>master</code> / <code>slave</code> / <code>none</code></td><td>yes</td><td></td></tr>
<tr><td>DB_NAME</td><td>Database name</td><td><code>schema-registry</code></td><td>yes</td><td></td></tr>
<tr><td>POD_NAME</td><td>(deprecated) used to promote to <code>master</code> role</td><td><code>schema1</code></td><td>no</td><td></td></tr>
<tr><td>EXPORT_DIR</td><td>Directory to save state of the database. The state is saved in newly created folder with timestamp</td><td><code>/var/db</code></td><td>no</td><td></td></tr>
<tr><td>IMPORT_FILE</td><td>JSON file from which SR should load initial state. If the state already exists this env variable will be ignored</td><td><code>/var/db/initial-schema.json</code></td><td>no</td><td></td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus requests</td><td>58105</td><td>no</td><td>58105</td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#kafka-configuration-1" id="kafka-configuration-1">Kafka Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>kafka</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>KAFKA_BROKERS</td><td>Address of Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td></td></tr>
<tr><td>KAFKA_GROUP_ID</td><td>Group ID of the consumer</td><td><code>schema_registry</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#amqp-configuration-1" id="amqp-configuration-1">AMQP Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>amqp</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>AMQP_CONNECTION_STRING</td><td>Connection URL to AMQP Server</td><td><code>amqp://user:CHANGEME@rabbitmq:5672/%2f</code></td><td>yes</td><td></td></tr>
<tr><td>AMQP_CONSUMER_TAG</td><td>Consumer tag</td><td><code>schema_registry</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#replication-configuration" id="replication-configuration">Replication Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> does NOT equal <code>grpc</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>REPLICATION_SOURCE</td><td>Kafka topic/AMQP queue</td><td><code>cdl.schema_registry.internal</code></td><td>yes</td><td></td></tr>
<tr><td>REPLICATION_DESTINATION</td><td>Kafka topic/AMQP exchange</td><td><code>cdl.schema_registry.internal</code></td><td>yes</td><td></td></tr>
</tbody></table>
<p>Mind that GRPC uses HTTP2 as its transport protocol (L4), so SCHEMA_REGISTRY_ADDR must be provided as <code>http://ip_or_name:port</code></p>
<h1><a class="header" href="#leader-elector" id="leader-elector">Leader Elector</a></h1>
<p>TODO</p>
<h1><a class="header" href="#ingestion-layer-1" id="ingestion-layer-1">Ingestion Layer</a></h1>
<p>Services in this layer are responsible for accepting generic messages from external systems via a message queue, validating them and and forwarding the message to correct repository.<br />
Currently consists only of the <a href="architecture/data_router.html">Data Router</a>. The <a href="architecture/data_router.html">Data Router</a> accepts messages in the following format:</p>
<pre><code class="language-json">{
  &quot;schemaId&quot;: &quot;ca435cee-2944-41f7-94ff-d1b26e99ba48&quot;,
  &quot;objectId&quot;: &quot;fc0b95e1-07eb-4bf8-b691-1a85a49ef8f0&quot;,
  &quot;data&quot;: { ...valid json object }
}
</code></pre>
<p>For more details, see the Data Router's <a href="architecture/data_router.html">readme</a>.</p>
<h1><a class="header" href="#data-router" id="data-router">Data Router</a></h1>
<h3><a class="header" href="#technical-description-2" id="technical-description-2">Technical Description</a></h3>
<p>The data router (internally <code>DR</code> is also used) is responsible for taking in input data and routing it to the correct storage based on 
the data's schema and its associated topic. </p>
<h3><a class="header" href="#communication" id="communication">Communication</a></h3>
<p>The data router routes requests from RabbitMQ and Kafka to the correct storage solution based on the schema and data type.
Topic and some of the basic configuration is obtained from Schema Registry. Data are routed and deposited onto configured queues.</p>
<p>Interacts with:</p>
<ul>
<li>Command Service (optional, either)</li>
<li>Message Queue (optional, either)</li>
<li>Schema Registry</li>
</ul>
<p>Ingest methods:</p>
<ul>
<li>Kafka</li>
</ul>
<p>Internal communication methods:</p>
<ul>
<li>Kafka (command-service)</li>
<li>gRPC (schema-registry)</li>
</ul>
<p>Below are the example data required by data router:</p>
<pre><code># high level description
{
    &quot;schemaId&quot;: &lt;UUID&gt;,
    &quot;objectId&quot;: &lt;UUID&gt;,
    &quot;data&quot;: { &quot;some_property&quot;: &quot;object&quot;}
}

# type description
{
    &quot;objectId&quot;(string) : (128bit valid uuid),
    &quot;schemaID&quot;(string) : (128bit valid uuid),
    &quot;data&quot;(string) : (array,dict,object,string, literally anything),
}

# example, minimalistic one liner
{ &quot;objectId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;schemaId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;data&quot;: {} }
</code></pre>
<p>Messages can be batched together, however please mind, that batched messages works best when used with the same schemaId.
Otherwise, messages will be split into sub-batches containing messages with the same schemaId</p>
<pre><code>[
  { &quot;objectId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;schemaId&quot;: f79d7ebd-4260-4919-9ba3-45ea6701f065, &quot;data&quot;: {} }
  { &quot;objectId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;schemaId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;data&quot;: {} }
  { &quot;objectId&quot;: 0369de4f-8025-4cf8-b6df-9446b51e4fd0, &quot;schemaId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;data&quot;: {} }
  { &quot;objectId&quot;: 0369de4f-8025-4cf8-b6df-9446b51e4fd0, &quot;schemaId&quot;: 07087162-e499-48f1-ad4a-cee7e77f1965, &quot;data&quot;: {} }
]
</code></pre>
<p>Please mind that internally, each message will get its own timestamp, with which data started being processed by CDL. This information is invisible for user.</p>
<h3><a class="header" href="#configuration-environment-variables-2" id="configuration-environment-variables-2">Configuration (Environment Variables)</a></h3>
<p>To configure the Data Router, set the following environment variables:</p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>COMMUNICATION_METHOD</td><td>The method of communication with external services</td><td><code>kafka</code> / <code>amqp</code> / <code>grpc</code></td><td>yes</td><td></td></tr>
<tr><td>INPUT_SOURCE</td><td>Kafka topic or AMQP queue</td><td><code>cdl.data.input</code></td><td>no, when <code>grpc</code> has been chosen</td><td></td></tr>
<tr><td>SCHEMA_REGISTRY_ADDR</td><td>Address of schema registry gRPC API</td><td>http://schema_registry:50101</td><td>yes</td><td></td></tr>
<tr><td>CACHE_CAPACITY</td><td>How many entries the cache can hold</td><td>1024</td><td>yes</td><td></td></tr>
<tr><td>TASK_LIMIT</td><td>Max requests handled in parallel</td><td>128</td><td>yes</td><td>128</td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus requests</td><td>58105</td><td>no</td><td>58105</td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#kafka-configuration-2" id="kafka-configuration-2">Kafka Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>kafka</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>KAFKA_BROKERS</td><td>Address of Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td></td></tr>
<tr><td>KAFKA_GROUP_ID</td><td>Group ID of the consumer</td><td><code>data_router</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#amqp-configuration-2" id="amqp-configuration-2">AMQP Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>amqp</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>AMQP_CONNECTION_STRING</td><td>Connection URL to AMQP Server</td><td><code>amqp://user:CHANGEME@rabbitmq:5672/%2f</code></td><td>yes</td><td></td></tr>
<tr><td>AMQP_CONSUMER_TAG</td><td>Consumer tag</td><td><code>data_router</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#grpc-configuration-1" id="grpc-configuration-1">gRPC Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>grpc</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>GRPC_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
</tbody></table>
<p>Mind that GRPC uses HTTP2 as its transport protocol (L4), so SCHEMA_REGISTRY_ADDR must be provided as <code>http://ip_or_name:port</code></p>
<p>See an example <a href="architecture/../deployment/index.html">configuration</a> of deployment of data router and other services.</p>
<h1><a class="header" href="#storage-layer-1" id="storage-layer-1">Storage Layer</a></h1>
<p>Consists of repositories for storing data.</p>
<p>Currently we support 2 types of repositories:</p>
<ul>
<li>Document
<ul>
<li>PostgreSQL</li>
</ul>
</li>
<li>Timeseries
<ul>
<li>Druid</li>
<li>Victoria Metrics</li>
</ul>
</li>
</ul>
<h1><a class="header" href="#command-services" id="command-services">Command Services</a></h1>
<p>Services that translate messages received from the <a href="architecture/data_router.html">Data Router</a> into their respective database's format. Currently only one Command Service implementation exists
and is built in such way that it can support multiple databases (one at a time).</p>
<h3><a class="header" href="#technical-description-3" id="technical-description-3">Technical Description</a></h3>
<p>The Command-Service (commonly refered also as <code>CS</code>, or <code>CSPG</code> - indicating posgres instance), interfaces storage repositories with the CDL ecosystem.</p>
<p>Interacts with:</p>
<ul>
<li>Data Router (optional, either)</li>
<li>Message Queue (optional, either)</li>
<li>Supported Repository (one of)</li>
</ul>
<p>Ingest methods:</p>
<ul>
<li>Kafka</li>
<li>RabbitMq</li>
<li>GRPC (currently either only one instance without kubernetes)</li>
</ul>
<p>Egest methods (supported repositories):</p>
<ul>
<li>Postgresql (tested on 12, should support anything &gt;=9, advised 13)</li>
<li>VictoriaMetrics</li>
<li>Druid</li>
<li>Sleight (CDL's document storage)</li>
<li>Troika (CDL's binary data repo)</li>
<li>.. or anything with matching GRPC :)</li>
</ul>
<h3><a class="header" href="#configuration-environment-variables-3" id="configuration-environment-variables-3">Configuration (Environment Variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>COMMUNICATION_METHOD</td><td>The method of communication with external services</td><td><code>kafka</code> / <code>amqp</code> / <code>grpc</code></td><td>yes</td><td></td></tr>
<tr><td>REPORT_DESTINATION</td><td>Kafka topic/AMQP exchange/callback URL to send notifications to (reporting disabled when empty)</td><td><code>cdl.notifications</code></td><td>no</td><td></td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus requests</td><td>58105</td><td>no</td><td>58105</td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#postgres-configuration" id="postgres-configuration">Postgres Configuration</a></h4>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>POSTGRES_USERNAME</td><td>Username</td><td><code>cdl</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_PASSWORD</td><td>Password</td><td><code>cdl1234</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_HOST</td><td>Host of the server</td><td><code>127.0.0.1</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_PORT</td><td>Port on which the server listens</td><td>5432</td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_DBNAME</td><td>Database name</td><td><code>cdl</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_SCHEMA</td><td>SQL Schema available for service</td><td><code>cdl</code></td><td>no</td><td><code>public</code></td></tr>
</tbody></table>
<h4><a class="header" href="#druid-configuration" id="druid-configuration">Druid Configuration</a></h4>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>DRUID_OUTPUT_BROKERS</td><td>Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td></td></tr>
<tr><td>DRUID_OUTPUT_TOPIC</td><td>Kafka topic</td><td><code>cdl.timeseries.internal.druid</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#victoria-metrics-configuration" id="victoria-metrics-configuration">Victoria Metrics Configuration</a></h4>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>VICTORIA_METRICS_OUTPUT_URL</td><td>Address of Victoria Metrics</td><td><code>http://victoria_metrics:8428</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#kafka-configuration-3" id="kafka-configuration-3">Kafka Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>kafka</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>KAFKA_BROKERS</td><td>Address of Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td></td></tr>
<tr><td>KAFKA_GROUP_ID</td><td>Group ID of the consumer</td><td><code>postgres_command</code></td><td>yes</td><td></td></tr>
<tr><td>ORDERED_SOURCES</td><td>Topics with ordered messages</td><td><code>cdl.timeseries.vm.1.data</code></td><td>no, but one of <code>ORDERED_SOURCES</code> and <code>UNORDERED_SOURCES</code> has to be present</td><td></td></tr>
<tr><td>UNORDERED_SOURCES</td><td>Topics with unordered messages</td><td><code>cdl.timeseries.vm.2.data</code></td><td>no, but one of <code>ORDERED_SOURCES</code> and <code>UNORDERED_SOURCES</code> has to be present</td><td></td></tr>
<tr><td>TASK_LIMIT</td><td>Max requests handled in parallel</td><td>32</td><td>yes</td><td>32</td></tr>
</tbody></table>
<h4><a class="header" href="#amqp-configuration-3" id="amqp-configuration-3">AMQP Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>amqp</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>AMQP_CONNECTION_STRING</td><td>Connection URL to AMQP Server</td><td><code>amqp://user:CHANGEME@rabbitmq:5672/%2f</code></td><td>yes</td><td></td></tr>
<tr><td>AMQP_CONSUMER_TAG</td><td>Consumer tag</td><td><code>postgres_command</code></td><td>yes</td><td></td></tr>
<tr><td>ORDERED_SOURCES</td><td>Queues with ordered messages</td><td><code>cdl.timeseries.vm.1.data</code></td><td>no, but one of <code>ORDERED_SOURCES</code> and <code>UNORDERED_SOURCES</code> has to be present</td><td></td></tr>
<tr><td>UNORDERED_SOURCES</td><td>Queues with unordered messages</td><td><code>cdl.timeseries.vm.2.data</code></td><td>no, but one of <code>ORDERED_SOURCES</code> and <code>UNORDERED_SOURCES</code> has to be present</td><td></td></tr>
<tr><td>TASK_LIMIT</td><td>Max requests handled in parallel</td><td>32</td><td>yes</td><td>32</td></tr>
</tbody></table>
<h4><a class="header" href="#grpc-configuration-2" id="grpc-configuration-2">gRPC Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>grpc</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>GRPC_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
<tr><td>REPORT_ENDPOINT_URL</td><td>URL to send notifications to</td><td><code>notifications:50102</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h1><a class="header" href="#db-shrinker-storage" id="db-shrinker-storage">Db Shrinker Storage</a></h1>
<h2><a class="header" href="#usage" id="usage">Usage</a></h2>
<p><code>db-shrinker-postgres &lt;connection-string&gt;</code></p>
<p>eg.</p>
<p><code>db-shrinker-postgres 'postgresql://postgres:1234@localhost:5432/postgres'</code></p>
<h2><a class="header" href="#description" id="description">Description</a></h2>
<p>This binary merges all versions of documents stored in PostgreSQL into one, 'most recent' version.
It handles whole and partial updates to documents in mention.</p>
<h2><a class="header" href="#testing" id="testing">Testing</a></h2>
<p>Currently only manual testing is supported.
You must have local postgres database provisioned for CDL document repository.</p>
<p>Setting up python env is done via <code>pip install -r tests/requirements.txt</code>.</p>
<p>Running <code>python data_loader.py</code> from tests directory should load sample data to your db. Just make sure that PSQL
connection string located in that file refers to your database instance.</p>
<p>After that you can run <code>db-shirnker-postgres</code> with same postgres connection string and compare data changed in your
database with <code>expected</code> entry in each test case.</p>
<h1><a class="header" href="#query-service" id="query-service">Query Service</a></h1>
<p>Each Query Service serves a common set of queries, and translates those into their respective database's query language.
Two query-services are present: one for timeseries databases, and one for documents.</p>
<h3><a class="header" href="#technical-description-4" id="technical-description-4">Technical Description</a></h3>
<p>The query service (<code>QS</code> or for example for postgresql its <code>QSPG</code>), is responsible for querying data from specific repository. It offers two paths that can be accessed:
First path depends on type of repo</p>
<h3><a class="header" href="#communication-1" id="communication-1">Communication</a></h3>
<p>Communication to query service is done through <a href="https://grpc.io/docs/what-is-grpc/introduction/">gRPC</a> based on two <a href="https://github.com/epiphany-platform/CommonDataLayer/tree/develop/crates/rpc/proto">endpoints</a> of querying for data by <code>SCHEMA_ID</code> or multiple <code>OBJECT_ID</code>s. Query service communicates with multiple databases such as postgresql, druid, victoria metrics. Query service also communicates with <a href="architecture/schema_registry.html">schema registry</a>. </p>
<p>Interacts with:</p>
<ul>
<li>Druid</li>
<li>Postgresql</li>
<li>VictoriaMetrics (accidentally also Prometheus)</li>
<li>Sled</li>
<li>Troika</li>
<li>.. any similar grpc-able repo</li>
</ul>
<p>Query methods:</p>
<ul>
<li>GRPC (req-response)</li>
</ul>
<p>Communication protocols:</p>
<ul>
<li>database specific</li>
</ul>
<h3><a class="header" href="#configuration-environment-variables-4" id="configuration-environment-variables-4">Configuration (Environment Variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INPUT_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus requests</td><td>58105</td><td>no</td><td>58105</td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#postgres-configuration-1" id="postgres-configuration-1">Postgres Configuration</a></h4>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>POSTGRES_USERNAME</td><td>Username</td><td><code>cdl</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_PASSWORD</td><td>Password</td><td><code>cdl1234</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_HOST</td><td>Host of the server</td><td><code>127.0.0.1</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_PORT</td><td>Port on which the server listens</td><td>5432</td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_DBNAME</td><td>Database name</td><td><code>cdl</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_SCHEMA</td><td>SQL Schema available for service</td><td><code>cdl</code></td><td>no</td><td><code>public</code></td></tr>
</tbody></table>
<p>See an example <a href="architecture/../deployment/index.html">configuration</a> of deployment of data router and other services. </p>
<h1><a class="header" href="#retrieval-layer-1" id="retrieval-layer-1">Retrieval Layer</a></h1>
<p>Services in this layer are responsible for responding on queries from external systems via REST.
Currently consists only of the <a href="architecture/query_router.html">Query Router</a>. </p>
<p>TODO: Query router JSON format.</p>
<h1><a class="header" href="#query-router" id="query-router">Query Router</a></h1>
<h3><a class="header" href="#technical-description-5" id="technical-description-5">Technical Description</a></h3>
<p>The Query Router (<code>QR</code>), is responsible for forwarding requests to specific query services. In CDL messages can be stored in any available repository, data router acts as a single entry point to multi-repo system and query router allows that data to be fetched easily.
Query Router first queries SR, then basing on received config, finds out specific QS that, hopefully, should be able to respond to specific query. Logic of that process is based on repo_type and query-service address stored with schema itself.</p>
<h3><a class="header" href="#communication-2" id="communication-2">Communication</a></h3>
<p>Interacts with:</p>
<ul>
<li>Query Service</li>
<li>Schema Registry</li>
</ul>
<p>Query methods:</p>
<ul>
<li>REST (request-response)</li>
</ul>
<p>Communication protocols:</p>
<ul>
<li>gRPC with query-services (request-response)</li>
<li>gRPC with schema-registry (request-response)</li>
</ul>
<h3><a class="header" href="#configuration-environment-variables-5" id="configuration-environment-variables-5">Configuration (Environment Variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INPUT_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
<tr><td>SCHEMA_REGISTRY_ADDR</td><td>Address of schema registry gRPC API</td><td>http://schema_registry:50101</td><td>yes</td><td></td></tr>
<tr><td>CACHE_CAPACITY</td><td>How many entries the cache can hold</td><td>1024</td><td>yes</td><td></td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus requests</td><td>58105</td><td>no</td><td>58105</td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h2><a class="header" href="#running-1" id="running-1">Running</a></h2>
<p>To run the <strong>query-router</strong> requires the <a href="architecture/schema_registry.html">Schema Registry</a> to be running and the <a href="architecture/query_service.html">Query Services</a> or the <a href="https://github.com/epiphany-platform/CommonDataLayer/tree/develop/crates/query-service-ts">Timeseries Query Services</a> connected to their respective repositories.</p>
<p><em>Note: Currently, the cache is valid forever: changing a schema's <strong>query-service</strong> address will not update in the <strong>query-router</strong>.</em></p>
<h2><a class="header" href="#functionality" id="functionality">Functionality</a></h2>
<p>REST API specification is available in <a href="https://github.com/epiphany-platform/CommonDataLayer/blob/develop/crates/query-router/api.yml">OpenAPI 3.0 spec</a>.</p>
<p>Currently, the <strong>query-router</strong> can:</p>
<ul>
<li>handle querying data by ID from document repositories,</li>
<li>query range of data by ID from time series repositories,</li>
<li>query data from repositories by SCHEMA_ID.</li>
</ul>
<p>Rough sketch of working process:
<img src="architecture/../mdbook-plantuml-img/9208ca32e9197d5d57535abc78a61451de003302.svg" alt="" /></p>
<h1><a class="header" href="#utils" id="utils">Utils</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
